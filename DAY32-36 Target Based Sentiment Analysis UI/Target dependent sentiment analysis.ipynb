{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98c54cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from data_utils import build_tokenizer, build_embedding_matrix, SentenceDataset,Tokenizer, Vocab\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn import metrics\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0618e764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10b1e53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading tokenizer: app/src/data/datasets/restaurants_tokenizer.dat\n",
      "loading word vectors...\n",
      "app/src/data/datasets/glove.twitter.27B.25d.txt\n"
     ]
    }
   ],
   "source": [
    "# data_files = ['../data/td_lstm_datasets/Laptops_Train.xml', '../data/td_lstm_datasets/Laptops_Test.xml']\n",
    "data_files = ['app/src/data/datasets/Laptops_Train.xml', 'app/src/data/datasets/Restaurants_Train.xml']\n",
    "tokenizer = build_tokenizer(\n",
    "    fnames=data_files,\n",
    "    max_length=80,\n",
    "    data_file='app/src/data/datasets/{0}_tokenizer.dat'.format('restaurants'))\n",
    "embedding_matrix = build_embedding_matrix(\n",
    "    vocab=tokenizer.vocab,\n",
    "    embed_dim=25,\n",
    "    data_file='app/src/data/datasets/{0}d_{1}_embedding_matrix.dat'.format('200', 'restaurants'))\n",
    "trainset = SentenceDataset(data_files[0] , tokenizer, target_dim=3)\n",
    "testset = SentenceDataset(data_files[1] , tokenizer, target_dim=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d33652",
   "metadata": {},
   "source": [
    "#### Parameters needs to be set before runnning this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "391bc2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 1\n",
    "lr=0.001\n",
    "l2_reg=1e-5\n",
    "num_epoch = 20\n",
    "input_cols = ['text']\n",
    "log_step = 5\n",
    "model_name = 'lstm'\n",
    "dataset = 'restaurant'\n",
    "batch_size = 64\n",
    "embed_dim = 200\n",
    "hidden_dim = 200\n",
    "polarities_dim = 3\n",
    "polarity_dict = {0: 'positive', 1: 'negative', 2:'neutral'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dc0b890",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset=trainset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset=testset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56866dfd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset :  {'positive': 994, 'negative': 871, 'neutral': 466}\n",
      "Testing dataset :  {'positive': 2164, 'negative': 808, 'neutral': 639}\n"
     ]
    }
   ],
   "source": [
    "polarity_count_train = {'positive':0, 'negative': 1, 'neutral':2}\n",
    "polarity_count_test = {'positive':0, 'negative': 1, 'neutral':2}\n",
    "for i in train_dataloader:\n",
    "    for polarity in [polarity_dict[int(j)] for j in i['polarity']]:\n",
    "        polarity_count_train[polarity] += 1\n",
    "for i in test_dataloader:\n",
    "    for polarity in [polarity_dict[int(j)] for j in i['polarity']]:\n",
    "        polarity_count_test[polarity] += 1\n",
    "print(\"Training dataset : \" , polarity_count_train)\n",
    "print(\"Testing dataset : \" , polarity_count_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d39eeab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicLSTM(nn.Module):\n",
    "    '''\n",
    "    LSTM which can hold variable length sequence, use like TensorFlow's RNN(input, lenght...).\n",
    "    '''\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, bias=True, batch_first=True, dropout=0,\n",
    "                 bidirectional=False, only_use_last_hidden_state=False, rnn_type='LSTM'):\n",
    "        super(DynamicLSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bias = bias\n",
    "        self.batch_first = batch_first\n",
    "        self.dropout = dropout\n",
    "        self.bidirectional = bidirectional\n",
    "        self.only_use_last_hidden_state = only_use_last_hidden_state\n",
    "        self.rnn_type = rnn_type\n",
    "        \n",
    "        if self.rnn_type == 'LSTM':\n",
    "            self.RNN = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers,\n",
    "                               bias=bias, batch_first=batch_first, dropout=dropout, bidirectional=bidirectional)\n",
    "        elif self.rnn_type == 'GRU':\n",
    "            self.RNN = nn.GRU(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers,\n",
    "                              bias=bias, batch_first=batch_first, dropout=dropout, bidirectional=bidirectional)\n",
    "        elif self.rnn_type == 'RNN':\n",
    "            self.RNN = nn.RNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers,\n",
    "                              bias=bias, batch_first=batch_first, dropout=dropout, bidirectional=bidirectional)\n",
    "    \n",
    "    def forward(self, x, x_len):\n",
    "        '''\n",
    "        sequence -> sort -> pad and pack -> process using RNN -> unpack -> unsort\n",
    "        '''\n",
    "        '''sort'''\n",
    "        x_sort_idx = torch.sort(x_len, descending=True)[1].long()\n",
    "        x_unsort_idx = torch.sort(x_sort_idx)[1].long()\n",
    "        x_len = x_len[x_sort_idx]\n",
    "        x = x[x_sort_idx]\n",
    "        '''pack'''\n",
    "        x_emb_p = torch.nn.utils.rnn.pack_padded_sequence(x, x_len, batch_first=self.batch_first)\n",
    "        ''' process '''\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            out_pack, (ht, ct) = self.RNN(x_emb_p, None)\n",
    "        else:\n",
    "            out_pack, ht = self.RNN(x_emb_p, None)\n",
    "            ct = None\n",
    "        '''unsort'''\n",
    "        ht = ht[:, x_unsort_idx]\n",
    "        if self.only_use_last_hidden_state:\n",
    "            return ht\n",
    "        else:\n",
    "            out, _ = torch.nn.utils.rnn.pad_packed_sequence(out_pack, batch_first=self.batch_first)\n",
    "            if self.batch_first:\n",
    "                out = out[x_unsort_idx]\n",
    "            else:\n",
    "                out = out[:, x_unsort_idx]\n",
    "            if self.rnn_type == 'LSTM':\n",
    "                ct = ct[:, x_unsort_idx]\n",
    "            return out, (ht, ct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dcde777",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    ''' Standard LSTM '''\n",
    "    def __init__(self, embedding_matrix):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.embed = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float))\n",
    "        self.lstm = DynamicLSTM(embed_dim, hidden_dim, num_layers=1, batch_first=True)\n",
    "        self.dense = nn.Linear(hidden_dim, polarities_dim)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        text = inputs[0]\n",
    "        x = self.embed(text)\n",
    "        x_len = torch.sum(text != 0, dim=-1)\n",
    "        _, (h_n, _) = self.lstm(x, x_len)\n",
    "        out = self.dense(h_n[0])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d8a8ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(embedding_matrix).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45033785",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=lr, weight_decay=l2_reg)\n",
    "writer = SummaryWriter(f\"runs/LSTM/BatchSize {batch_size} LR {lr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0737e1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_params(model):\n",
    "    for p in model.parameters():\n",
    "        if p.requires_grad:\n",
    "            if len(p.shape) > 1:\n",
    "                torch.nn.init.xavier_normal_(p)\n",
    "            else:\n",
    "                stdv = 1. / (p.shape[0]**0.5)\n",
    "                torch.nn.init.uniform_(p, a=-stdv, b=stdv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd163516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_trainable_params: 322203, n_nontrainable_params: 141750\n"
     ]
    }
   ],
   "source": [
    "n_trainable_params, n_nontrainable_params = 0, 0\n",
    "for p in model.parameters():\n",
    "    n_params = torch.prod(torch.tensor(p.shape))\n",
    "    if p.requires_grad:\n",
    "        n_trainable_params += n_params\n",
    "    else:\n",
    "        n_nontrainable_params += n_params\n",
    "print('n_trainable_params: {0}, n_nontrainable_params: {1}'.format(n_trainable_params, n_nontrainable_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aaebe45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, writer, max_test_acc_overall=0, model_name='LSTM'):\n",
    "    max_test_acc = 0\n",
    "    max_f1 = 0\n",
    "    global_step = 0\n",
    "    for epoch in range(num_epoch):\n",
    "        print('>' * 50)\n",
    "        print('epoch:', epoch)\n",
    "        n_correct, n_total = 0, 0\n",
    "        for i_batch, sample_batched in enumerate(train_dataloader):\n",
    "            global_step += 1\n",
    "            # switch model to training mode, clear gradient accumulators\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            inputs = [sample_batched[col].to(device) for col in input_cols]\n",
    "            outputs = model(inputs)\n",
    "            targets = sample_batched['polarity'].to(device)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            writer.add_scalar(\"Training loss\", loss, global_step=global_step)\n",
    "            \n",
    "\n",
    "            if global_step % log_step == 0:\n",
    "                n_correct += (torch.argmax(outputs, -1) == targets).sum().item()\n",
    "                n_total += len(outputs)\n",
    "                train_acc = n_correct / n_total\n",
    "                writer.add_scalar(\"Training Accuracy\", \n",
    "                                  train_acc,\n",
    "                                  global_step=global_step)\n",
    "                test_acc, f1 = evaluate(model, writer, global_step)\n",
    "                if test_acc > max_test_acc:\n",
    "                    max_test_acc = test_acc\n",
    "                    if test_acc > max_test_acc_overall:\n",
    "                        if not os.path.exists('state_dict'):\n",
    "                            os.mkdir('state_dict')\n",
    "                        path = './state_dict/{0}_{1}_{2}class_acc{3:.4f}'.format(model_name, dataset, polarities_dim, test_acc)\n",
    "                        torch.save(model.state_dict(), path)\n",
    "                        print('model saved:', path)\n",
    "                if f1 > max_f1:\n",
    "                    max_f1 = f1\n",
    "                print('loss: {:.4f}, acc: {:.4f}, test_acc: {:.4f}, f1: {:.4f}'.format(loss.item(), train_acc, test_acc, f1))\n",
    "    return max_test_acc, max_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61c36b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, writer, step):\n",
    "    # switch model to evaluation mode\n",
    "    model.eval()\n",
    "    n_test_correct, n_test_total = 0, 0\n",
    "    t_targets_all, t_outputs_all = None, None\n",
    "    with torch.no_grad():\n",
    "        for t_batch, t_sample_batched in enumerate(test_dataloader):\n",
    "            t_inputs = [t_sample_batched[col].to(device) for col in input_cols]\n",
    "            t_targets = t_sample_batched['polarity'].to(device)\n",
    "            t_outputs = model(t_inputs)\n",
    "\n",
    "            n_test_correct += (torch.argmax(t_outputs, -1) == t_targets).sum().item()\n",
    "            n_test_total += len(t_outputs)\n",
    "\n",
    "            t_targets_all = torch.cat((t_targets_all, t_targets), dim=0) if t_targets_all is not None else t_targets\n",
    "            t_outputs_all = torch.cat((t_outputs_all, t_outputs), dim=0) if t_outputs_all is not None else t_outputs\n",
    "    test_acc = n_test_correct / n_test_total\n",
    "    writer.add_scalar(\"Testing Accuracy\", \n",
    "                                  test_acc,\n",
    "                                  global_step=step)\n",
    "    f1 = metrics.f1_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[0, 1, 2], average='macro')\n",
    "    return test_acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa9bf891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(model, writer):\n",
    "    max_test_acc_overall = 0\n",
    "    max_f1_overall = 0\n",
    "    repeats = 1\n",
    "    for i in range(repeats):\n",
    "        print('repeat:', i)\n",
    "        reset_params(model)\n",
    "        max_test_acc, max_f1 = train(model, criterion, optimizer, writer, max_test_acc_overall)\n",
    "        print('max_test_acc: {0}, max_f1: {1}'.format(max_test_acc, max_f1))\n",
    "        max_test_acc_overall = max(max_test_acc, max_test_acc_overall)\n",
    "        max_f1_overall = max(max_f1, max_f1_overall)\n",
    "        print('#' * 50)\n",
    "    print('max_test_acc_overall:', max_test_acc_overall)\n",
    "    print('max_f1_overall:', max_f1_overall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9cf8bbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repeat: 0\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "epoch: 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 200, got 25",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-8b9533f58dd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-d54026b398c4>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(model, writer)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'repeat:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mreset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mmax_test_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_test_acc_overall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'max_test_acc: {0}, max_f1: {1}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_test_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_f1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mmax_test_acc_overall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_test_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_test_acc_overall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-b31384708966>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, criterion, optimizer, writer, max_test_acc_overall, model_name)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msample_batched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_batched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'polarity'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-cf4020823dd4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mx_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_n\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-b423b87e3411>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, x_len)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;34m''' process '''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'LSTM'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mout_pack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mht\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mct\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_emb_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mout_pack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mht\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_emb_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    618\u001b[0m                            \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m                            ):\n\u001b[0;32m--> 620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m         self.check_hidden_size(hidden[0], self.get_expected_hidden_size(input, batch_sizes),\n\u001b[1;32m    622\u001b[0m                                'Expected hidden[0] size {}, got {}')\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    203\u001b[0m                     expected_input_dim, input.dim()))\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m    206\u001b[0m                 'input.size(-1) must be equal to input_size. Expected {}, got {}'.format(\n\u001b[1;32m    207\u001b[0m                     self.input_size, input.size(-1)))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 200, got 25"
     ]
    }
   ],
   "source": [
    "run(model, writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08441c0",
   "metadata": {},
   "source": [
    "### Run the latest saved model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdabae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_file = sorted([os.path.join('state_dict',path) for path in os.listdir('state_dict')], key=os.path.getmtime)[-1]\n",
    "checkpoints = torch.load(latest_file)\n",
    "model.load_state_dict(checkpoints)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad85a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = torch.tensor(tokenizer.text_to_sequence(\"Keyboard is great, very quiet for all the typing that I do.\"))\n",
    "output = model(sample_data.reshape(1,1,-1))\n",
    "polarity_dict[int(torch.argmax(output, -1))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961e36a7",
   "metadata": {},
   "source": [
    "#### Parameters needs to be set before runnning this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0ec42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 1\n",
    "lr=0.001\n",
    "l2_reg=1e-5\n",
    "num_epoch = 20\n",
    "input_cols = ['text', 'aspect']\n",
    "log_step = 5\n",
    "model_name = 'ae_lstm'\n",
    "dataset = 'restaurant'\n",
    "batch_size = 64\n",
    "embed_dim = 200\n",
    "hidden_dim = 200\n",
    "polarities_dim = 3\n",
    "polarity_dict = {0: 'positive', 1: 'negative', 2:'neutral'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfad139",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SqueezeEmbedding(nn.Module):\n",
    "    '''\n",
    "    Squeeze sequence embedding length to the longest one in the batch\n",
    "    '''\n",
    "    def __init__(self, batch_first=True):\n",
    "        super(SqueezeEmbedding, self).__init__()\n",
    "        self.batch_first = batch_first\n",
    "    \n",
    "    def forward(self, x, x_len):\n",
    "        '''\n",
    "        sequence -> sort -> pad and pack -> unpack -> unsort\n",
    "        '''\n",
    "        '''sort'''\n",
    "        x_sort_idx = torch.sort(x_len, descending=True)[1].long()\n",
    "        x_unsort_idx = torch.sort(x_sort_idx)[1].long()\n",
    "        x_len = x_len[x_sort_idx]\n",
    "        x = x[x_sort_idx]\n",
    "        '''pack'''\n",
    "        x_emb_p = torch.nn.utils.rnn.pack_padded_sequence(x, x_len, batch_first=self.batch_first)\n",
    "        '''unpack'''\n",
    "        out, _ = torch.nn.utils.rnn.pad_packed_sequence(x_emb_p, batch_first=self.batch_first)\n",
    "        if self.batch_first:\n",
    "            out = out[x_unsort_idx]\n",
    "        else:\n",
    "            out = out[:, x_unsort_idx]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd855db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE_LSTM(nn.Module):\n",
    "    ''' LSTM with Aspect Embedding '''\n",
    "    def __init__(self, embedding_matrix):\n",
    "        super(AE_LSTM, self).__init__()\n",
    "        self.embed = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float))\n",
    "        self.squeeze_embedding = SqueezeEmbedding()\n",
    "        self.lstm = DynamicLSTM(embed_dim*2, hidden_dim, num_layers=1, batch_first=True)\n",
    "        self.dense = nn.Linear(hidden_dim, polarities_dim)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        text, aspect_text = inputs[0], inputs[1]\n",
    "        x_len = torch.sum(text != 0, dim=-1)\n",
    "        x_len_max = torch.max(x_len)\n",
    "        aspect_len = torch.sum(aspect_text != 0, dim=-1).float()\n",
    "        \n",
    "        x = self.embed(text)\n",
    "        x = self.squeeze_embedding(x, x_len)\n",
    "        aspect = self.embed(aspect_text)\n",
    "        aspect_pool = torch.div(torch.sum(aspect, dim=1), aspect_len.view(aspect_len.size(0), 1))\n",
    "        aspect = torch.unsqueeze(aspect_pool, dim=1).expand(-1, x_len_max, -1)\n",
    "        x = torch.cat((aspect, x), dim=-1)\n",
    "        \n",
    "        _, (h_n, _) = self.lstm(x, x_len)\n",
    "        out = self.dense(h_n[0])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fd1f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_AE = AE_LSTM(embedding_matrix).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f4c27f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aef43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "params = filter(lambda p: p.requires_grad, model_AE.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=lr, weight_decay=l2_reg)\n",
    "writer_AE = SummaryWriter(f\"runs/AE_LSTM/BatchSize {batch_size} LR {lr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cff074",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run(model_AE , writer_AE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c540fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = torch.tensor(tokenizer.text_to_sequence(\"MS Office 2011 for Mac is wonderful, well worth it.\")).reshape(1,-1)\n",
    "sample_aspect = torch.tensor(tokenizer.text_to_sequence('MS Office 2011 for Mac').reshape(1,-1))\n",
    "data = [sample_data, sample_aspect]\n",
    "output = model_AE(data)\n",
    "polarity_dict[int(torch.argmax(output, -1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18450b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 80\n",
    "position_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a68076c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39b8045",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PBAN(nn.Module):\n",
    "    ''' Position-aware bidirectional attention network '''\n",
    "    def __init__(self, embedding_matrix):\n",
    "        super(PBAN, self).__init__()\n",
    "        self.text_embed = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float))\n",
    "        self.pos_embed = nn.Embedding(max_length, position_dim)\n",
    "        self.left_gru = DynamicLSTM(embed_dim, hidden_dim, num_layers=1, \n",
    "                                    batch_first=True, bidirectional=True, rnn_type='GRU')\n",
    "        self.right_gru = DynamicLSTM(embed_dim+position_dim, hidden_dim, num_layers=1, \n",
    "                                     batch_first=True, bidirectional=True, rnn_type='GRU')\n",
    "        self.weight_m = nn.Parameter(torch.Tensor(hidden_dim*2, hidden_dim*2))\n",
    "        self.bias_m = nn.Parameter(torch.Tensor(1))\n",
    "        self.weight_n = nn.Parameter(torch.Tensor(hidden_dim*2, hidden_dim*2))\n",
    "        self.bias_n = nn.Parameter(torch.Tensor(1))\n",
    "        self.w_r = nn.Linear(hidden_dim*2, hidden_dim)\n",
    "        self.w_s = nn.Linear(hidden_dim, polarities_dim)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        text, aspect_text, position_tag = inputs[0], inputs[1], inputs[2]\n",
    "        ''' Sentence representation '''\n",
    "        x = self.text_embed(text)\n",
    "        position = self.pos_embed(position_tag)\n",
    "        x_len = torch.sum(text != 0, dim=-1)\n",
    "        x = torch.cat((position, x), dim=-1)\n",
    "        h_x, _ = self.right_gru(x, x_len)\n",
    "        ''' Aspect term representation '''\n",
    "        aspect = self.text_embed(aspect_text)\n",
    "        aspect_len = torch.sum(aspect_text != 0, dim=-1)\n",
    "        h_t, _ = self.left_gru(aspect, aspect_len)\n",
    "        ''' Aspect term to position-aware sentence attention '''\n",
    "        alpha = F.softmax(torch.tanh(torch.add(torch.bmm(torch.matmul(h_t, self.weight_m), torch.transpose(h_x, 1, 2)), self.bias_m)), dim=1)\n",
    "        s_x = torch.bmm(alpha, h_x)\n",
    "        ''' Position-aware sentence attention to aspect term '''\n",
    "        h_x_pool = torch.unsqueeze(torch.div(torch.sum(h_x, dim=1), x_len.float().view(x_len.size(0), 1)), dim=1)\n",
    "        gamma = F.softmax(torch.tanh(torch.add(torch.bmm(torch.matmul(h_x_pool, self.weight_n), torch.transpose(h_t, 1, 2)), self.bias_n)), dim=1)\n",
    "        h_r = torch.squeeze(torch.bmm(gamma, s_x), dim=1)\n",
    "        ''' Output transform '''\n",
    "        out = torch.tanh(self.w_r(h_r))\n",
    "        out = self.w_s(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f744fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_PBAN = PBAN(embedding_matrix).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1551dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_PBAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9756fee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 1\n",
    "lr=0.001\n",
    "l2_reg=1e-5\n",
    "num_epoch = 20\n",
    "input_cols = ['text', 'aspect', 'position']\n",
    "log_step = 5\n",
    "model_name = 'pban_lstm'\n",
    "dataset = 'restaurant'\n",
    "batch_size = 64\n",
    "embed_dim = 200\n",
    "hidden_dim = 200\n",
    "polarities_dim = 3\n",
    "polarity_dict = {0: 'positive', 1: 'negative', 2:'neutral'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fff5dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "params = filter(lambda p: p.requires_grad, model_PBAN.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=lr, weight_decay=l2_reg)\n",
    "writer_PBAN = SummaryWriter(f\"runs/PBAN_LSTM/BatchSize {batch_size} LR {lr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a833d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "run(model_PBAN , writer_PBAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48db372",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0973ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ac2f01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
