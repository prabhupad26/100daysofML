{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98c54cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from data_utils import build_tokenizer, build_embedding_matrix, SentenceDataset,Tokenizer, Vocab\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn import metrics\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0618e764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10b1e53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End\n",
      "loading word vectors...\n",
      "data/datasets/glove.twitter.27B.200d.txt\n"
     ]
    }
   ],
   "source": [
    "data_files = ['data/datasets/Laptops_Train.xml', 'data/datasets/Laptops_Test.xml']\n",
    "# data_files = ['data/datasets/Restaurants_Train.xml', 'data/datasets/Restaurants_Test.xml']\n",
    "tokenizer = build_tokenizer(\n",
    "    fnames=data_files,\n",
    "    max_length=80,\n",
    "    data_file='data/datasets/{0}_tokenizer.dat'.format('laptops'))\n",
    "embedding_matrix = build_embedding_matrix(\n",
    "    vocab=tokenizer.vocab,\n",
    "    embed_dim=200,\n",
    "    data_file='data/datasets/{0}d_{1}_embedding_matrix.dat'.format('200', 'laptops'))\n",
    "trainset = SentenceDataset(data_files[0] , tokenizer, target_dim=3)\n",
    "testset = SentenceDataset(data_files[1] , tokenizer, target_dim=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb43216d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "835c0500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<data_utils.Tokenizer at 0x7f0d9084aa60>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle.load(open('data/datasets/laptops_tokenizer.dat','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d33652",
   "metadata": {},
   "source": [
    "#### Parameters needs to be set before runnning this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391bc2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 1\n",
    "lr=0.001\n",
    "l2_reg=1e-5\n",
    "num_epoch = 20\n",
    "input_cols = ['text']\n",
    "log_step = 5\n",
    "model_name = 'lstm'\n",
    "dataset = 'restaurant'\n",
    "batch_size = 64\n",
    "embed_dim = 200\n",
    "hidden_dim = 200\n",
    "polarities_dim = 3\n",
    "polarity_dict = {0: 'positive', 1: 'negative', 2:'neutral'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc0b890",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset=trainset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset=testset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56866dfd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "polarity_count_train = {'positive':0, 'negative': 1, 'neutral':2}\n",
    "polarity_count_test = {'positive':0, 'negative': 1, 'neutral':2}\n",
    "for i in train_dataloader:\n",
    "    for polarity in [polarity_dict[int(j)] for j in i['polarity']]:\n",
    "        polarity_count_train[polarity] += 1\n",
    "for i in test_dataloader:\n",
    "    for polarity in [polarity_dict[int(j)] for j in i['polarity']]:\n",
    "        polarity_count_test[polarity] += 1\n",
    "print(\"Training dataset : \" , polarity_count_train)\n",
    "print(\"Testing dataset : \" , polarity_count_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39eeab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicLSTM(nn.Module):\n",
    "    '''\n",
    "    LSTM which can hold variable length sequence, use like TensorFlow's RNN(input, lenght...).\n",
    "    '''\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1, bias=True, batch_first=True, dropout=0,\n",
    "                 bidirectional=False, only_use_last_hidden_state=False, rnn_type='LSTM'):\n",
    "        super(DynamicLSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bias = bias\n",
    "        self.batch_first = batch_first\n",
    "        self.dropout = dropout\n",
    "        self.bidirectional = bidirectional\n",
    "        self.only_use_last_hidden_state = only_use_last_hidden_state\n",
    "        self.rnn_type = rnn_type\n",
    "        \n",
    "        if self.rnn_type == 'LSTM':\n",
    "            self.RNN = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers,\n",
    "                               bias=bias, batch_first=batch_first, dropout=dropout, bidirectional=bidirectional)\n",
    "        elif self.rnn_type == 'GRU':\n",
    "            self.RNN = nn.GRU(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers,\n",
    "                              bias=bias, batch_first=batch_first, dropout=dropout, bidirectional=bidirectional)\n",
    "        elif self.rnn_type == 'RNN':\n",
    "            self.RNN = nn.RNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers,\n",
    "                              bias=bias, batch_first=batch_first, dropout=dropout, bidirectional=bidirectional)\n",
    "    \n",
    "    def forward(self, x, x_len):\n",
    "        '''\n",
    "        sequence -> sort -> pad and pack -> process using RNN -> unpack -> unsort\n",
    "        '''\n",
    "        '''sort'''\n",
    "        x_sort_idx = torch.sort(x_len, descending=True)[1].long()\n",
    "        x_unsort_idx = torch.sort(x_sort_idx)[1].long()\n",
    "        x_len = x_len[x_sort_idx]\n",
    "        x = x[x_sort_idx]\n",
    "        '''pack'''\n",
    "        x_emb_p = torch.nn.utils.rnn.pack_padded_sequence(x, x_len, batch_first=self.batch_first)\n",
    "        ''' process '''\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            out_pack, (ht, ct) = self.RNN(x_emb_p, None)\n",
    "        else:\n",
    "            out_pack, ht = self.RNN(x_emb_p, None)\n",
    "            ct = None\n",
    "        '''unsort'''\n",
    "        ht = ht[:, x_unsort_idx]\n",
    "        if self.only_use_last_hidden_state:\n",
    "            return ht\n",
    "        else:\n",
    "            out, _ = torch.nn.utils.rnn.pad_packed_sequence(out_pack, batch_first=self.batch_first)\n",
    "            if self.batch_first:\n",
    "                out = out[x_unsort_idx]\n",
    "            else:\n",
    "                out = out[:, x_unsort_idx]\n",
    "            if self.rnn_type == 'LSTM':\n",
    "                ct = ct[:, x_unsort_idx]\n",
    "            return out, (ht, ct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcde777",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    ''' Standard LSTM '''\n",
    "    def __init__(self, embedding_matrix):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.embed = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float))\n",
    "        self.lstm = DynamicLSTM(embed_dim, hidden_dim, num_layers=1, batch_first=True)\n",
    "        self.dense = nn.Linear(hidden_dim, polarities_dim)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        text = inputs[0]\n",
    "        x = self.embed(text)\n",
    "        x_len = torch.sum(text != 0, dim=-1)\n",
    "        _, (h_n, _) = self.lstm(x, x_len)\n",
    "        out = self.dense(h_n[0])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8a8ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(embedding_matrix).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45033785",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=lr, weight_decay=l2_reg)\n",
    "writer = SummaryWriter(f\"runs/LSTM/BatchSize {batch_size} LR {lr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0737e1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_params(model):\n",
    "    for p in model.parameters():\n",
    "        if p.requires_grad:\n",
    "            if len(p.shape) > 1:\n",
    "                torch.nn.init.xavier_normal_(p)\n",
    "            else:\n",
    "                stdv = 1. / (p.shape[0]**0.5)\n",
    "                torch.nn.init.uniform_(p, a=-stdv, b=stdv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd163516",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trainable_params, n_nontrainable_params = 0, 0\n",
    "for p in model.parameters():\n",
    "    n_params = torch.prod(torch.tensor(p.shape))\n",
    "    if p.requires_grad:\n",
    "        n_trainable_params += n_params\n",
    "    else:\n",
    "        n_nontrainable_params += n_params\n",
    "print('n_trainable_params: {0}, n_nontrainable_params: {1}'.format(n_trainable_params, n_nontrainable_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaebe45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, writer, max_test_acc_overall=0, model_name='LSTM'):\n",
    "    max_test_acc = 0\n",
    "    max_f1 = 0\n",
    "    global_step = 0\n",
    "    for epoch in range(num_epoch):\n",
    "        print('>' * 50)\n",
    "        print('epoch:', epoch)\n",
    "        n_correct, n_total = 0, 0\n",
    "        for i_batch, sample_batched in enumerate(train_dataloader):\n",
    "            global_step += 1\n",
    "            # switch model to training mode, clear gradient accumulators\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            inputs = [sample_batched[col].to(device) for col in input_cols]\n",
    "            outputs = model(inputs)\n",
    "            targets = sample_batched['polarity'].to(device)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            writer.add_scalar(\"Training loss\", loss, global_step=global_step)\n",
    "            \n",
    "\n",
    "            if global_step % log_step == 0:\n",
    "                n_correct += (torch.argmax(outputs, -1) == targets).sum().item()\n",
    "                n_total += len(outputs)\n",
    "                train_acc = n_correct / n_total\n",
    "                writer.add_scalar(\"Training Accuracy\", \n",
    "                                  train_acc,\n",
    "                                  global_step=global_step)\n",
    "                test_acc, f1 = evaluate(model, writer, global_step)\n",
    "                if test_acc > max_test_acc:\n",
    "                    max_test_acc = test_acc\n",
    "                    if test_acc > max_test_acc_overall:\n",
    "                        if not os.path.exists('state_dict'):\n",
    "                            os.mkdir('state_dict')\n",
    "                        path = './state_dict/{0}_{1}_{2}class_acc{3:.4f}'.format(model_name, dataset, polarities_dim, test_acc)\n",
    "                        torch.save(model.state_dict(), path)\n",
    "                        print('model saved:', path)\n",
    "                if f1 > max_f1:\n",
    "                    max_f1 = f1\n",
    "                print('loss: {:.4f}, acc: {:.4f}, test_acc: {:.4f}, f1: {:.4f}'.format(loss.item(), train_acc, test_acc, f1))\n",
    "    return max_test_acc, max_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c36b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, writer, step):\n",
    "    # switch model to evaluation mode\n",
    "    model.eval()\n",
    "    n_test_correct, n_test_total = 0, 0\n",
    "    t_targets_all, t_outputs_all = None, None\n",
    "    with torch.no_grad():\n",
    "        for t_batch, t_sample_batched in enumerate(test_dataloader):\n",
    "            t_inputs = [t_sample_batched[col].to(device) for col in input_cols]\n",
    "            t_targets = t_sample_batched['polarity'].to(device)\n",
    "            t_outputs = model(t_inputs)\n",
    "\n",
    "            n_test_correct += (torch.argmax(t_outputs, -1) == t_targets).sum().item()\n",
    "            n_test_total += len(t_outputs)\n",
    "\n",
    "            t_targets_all = torch.cat((t_targets_all, t_targets), dim=0) if t_targets_all is not None else t_targets\n",
    "            t_outputs_all = torch.cat((t_outputs_all, t_outputs), dim=0) if t_outputs_all is not None else t_outputs\n",
    "    test_acc = n_test_correct / n_test_total\n",
    "    writer.add_scalar(\"Testing Accuracy\", \n",
    "                                  test_acc,\n",
    "                                  global_step=step)\n",
    "    f1 = metrics.f1_score(t_targets_all.cpu(), torch.argmax(t_outputs_all, -1).cpu(), labels=[0, 1, 2], average='macro')\n",
    "    return test_acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9bf891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(model, writer):\n",
    "    max_test_acc_overall = 0\n",
    "    max_f1_overall = 0\n",
    "    repeats = 1\n",
    "    for i in range(repeats):\n",
    "        print('repeat:', i)\n",
    "        reset_params(model)\n",
    "        max_test_acc, max_f1 = train(model, criterion, optimizer, writer, max_test_acc_overall)\n",
    "        print('max_test_acc: {0}, max_f1: {1}'.format(max_test_acc, max_f1))\n",
    "        max_test_acc_overall = max(max_test_acc, max_test_acc_overall)\n",
    "        max_f1_overall = max(max_f1, max_f1_overall)\n",
    "        print('#' * 50)\n",
    "    print('max_test_acc_overall:', max_test_acc_overall)\n",
    "    print('max_f1_overall:', max_f1_overall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf8bbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "run(model, writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08441c0",
   "metadata": {},
   "source": [
    "### Run the latest saved model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdabae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_file = sorted([os.path.join('state_dict',path) for path in os.listdir('state_dict')], key=os.path.getmtime)[-1]\n",
    "checkpoints = torch.load(latest_file)\n",
    "model.load_state_dict(checkpoints)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad85a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = torch.tensor(tokenizer.text_to_sequence(\"Keyboard is great, very quiet for all the typing that I do.\"))\n",
    "output = model(sample_data.reshape(1,1,-1))\n",
    "polarity_dict[int(torch.argmax(output, -1))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961e36a7",
   "metadata": {},
   "source": [
    "#### Parameters needs to be set before runnning this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0ec42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 1\n",
    "lr=0.001\n",
    "l2_reg=1e-5\n",
    "num_epoch = 20\n",
    "input_cols = ['text', 'aspect']\n",
    "log_step = 5\n",
    "model_name = 'ae_lstm'\n",
    "dataset = 'restaurant'\n",
    "batch_size = 64\n",
    "embed_dim = 200\n",
    "hidden_dim = 200\n",
    "polarities_dim = 3\n",
    "polarity_dict = {0: 'positive', 1: 'negative', 2:'neutral'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfad139",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SqueezeEmbedding(nn.Module):\n",
    "    '''\n",
    "    Squeeze sequence embedding length to the longest one in the batch\n",
    "    '''\n",
    "    def __init__(self, batch_first=True):\n",
    "        super(SqueezeEmbedding, self).__init__()\n",
    "        self.batch_first = batch_first\n",
    "    \n",
    "    def forward(self, x, x_len):\n",
    "        '''\n",
    "        sequence -> sort -> pad and pack -> unpack -> unsort\n",
    "        '''\n",
    "        '''sort'''\n",
    "        x_sort_idx = torch.sort(x_len, descending=True)[1].long()\n",
    "        x_unsort_idx = torch.sort(x_sort_idx)[1].long()\n",
    "        x_len = x_len[x_sort_idx]\n",
    "        x = x[x_sort_idx]\n",
    "        '''pack'''\n",
    "        x_emb_p = torch.nn.utils.rnn.pack_padded_sequence(x, x_len, batch_first=self.batch_first)\n",
    "        '''unpack'''\n",
    "        out, _ = torch.nn.utils.rnn.pad_packed_sequence(x_emb_p, batch_first=self.batch_first)\n",
    "        if self.batch_first:\n",
    "            out = out[x_unsort_idx]\n",
    "        else:\n",
    "            out = out[:, x_unsort_idx]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd855db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE_LSTM(nn.Module):\n",
    "    ''' LSTM with Aspect Embedding '''\n",
    "    def __init__(self, embedding_matrix):\n",
    "        super(AE_LSTM, self).__init__()\n",
    "        self.embed = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float))\n",
    "        self.squeeze_embedding = SqueezeEmbedding()\n",
    "        self.lstm = DynamicLSTM(embed_dim*2, hidden_dim, num_layers=1, batch_first=True)\n",
    "        self.dense = nn.Linear(hidden_dim, polarities_dim)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        text, aspect_text = inputs[0], inputs[1]\n",
    "        x_len = torch.sum(text != 0, dim=-1)\n",
    "        x_len_max = torch.max(x_len)\n",
    "        aspect_len = torch.sum(aspect_text != 0, dim=-1).float()\n",
    "        \n",
    "        x = self.embed(text)\n",
    "        x = self.squeeze_embedding(x, x_len)\n",
    "        aspect = self.embed(aspect_text)\n",
    "        aspect_pool = torch.div(torch.sum(aspect, dim=1), aspect_len.view(aspect_len.size(0), 1))\n",
    "        aspect = torch.unsqueeze(aspect_pool, dim=1).expand(-1, x_len_max, -1)\n",
    "        x = torch.cat((aspect, x), dim=-1)\n",
    "        \n",
    "        _, (h_n, _) = self.lstm(x, x_len)\n",
    "        out = self.dense(h_n[0])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fd1f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_AE = AE_LSTM(embedding_matrix).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f4c27f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aef43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "params = filter(lambda p: p.requires_grad, model_AE.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=lr, weight_decay=l2_reg)\n",
    "writer_AE = SummaryWriter(f\"runs/AE_LSTM/BatchSize {batch_size} LR {lr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cff074",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run(model_AE , writer_AE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c540fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = torch.tensor(tokenizer.text_to_sequence(\"MS Office 2011 for Mac is wonderful, well worth it.\")).reshape(1,-1)\n",
    "sample_aspect = torch.tensor(tokenizer.text_to_sequence('MS Office 2011 for Mac').reshape(1,-1))\n",
    "data = [sample_data, sample_aspect]\n",
    "output = model_AE(data)\n",
    "polarity_dict[int(torch.argmax(output, -1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18450b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 80\n",
    "position_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a68076c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39b8045",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PBAN(nn.Module):\n",
    "    ''' Position-aware bidirectional attention network '''\n",
    "    def __init__(self, embedding_matrix):\n",
    "        super(PBAN, self).__init__()\n",
    "        self.text_embed = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float))\n",
    "        self.pos_embed = nn.Embedding(max_length, position_dim)\n",
    "        self.left_gru = DynamicLSTM(embed_dim, hidden_dim, num_layers=1, \n",
    "                                    batch_first=True, bidirectional=True, rnn_type='GRU')\n",
    "        self.right_gru = DynamicLSTM(embed_dim+position_dim, hidden_dim, num_layers=1, \n",
    "                                     batch_first=True, bidirectional=True, rnn_type='GRU')\n",
    "        self.weight_m = nn.Parameter(torch.Tensor(hidden_dim*2, hidden_dim*2))\n",
    "        self.bias_m = nn.Parameter(torch.Tensor(1))\n",
    "        self.weight_n = nn.Parameter(torch.Tensor(hidden_dim*2, hidden_dim*2))\n",
    "        self.bias_n = nn.Parameter(torch.Tensor(1))\n",
    "        self.w_r = nn.Linear(hidden_dim*2, hidden_dim)\n",
    "        self.w_s = nn.Linear(hidden_dim, polarities_dim)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        text, aspect_text, position_tag = inputs[0], inputs[1], inputs[2]\n",
    "        ''' Sentence representation '''\n",
    "        x = self.text_embed(text)\n",
    "        position = self.pos_embed(position_tag)\n",
    "        x_len = torch.sum(text != 0, dim=-1)\n",
    "        x = torch.cat((position, x), dim=-1)\n",
    "        h_x, _ = self.right_gru(x, x_len)\n",
    "        ''' Aspect term representation '''\n",
    "        aspect = self.text_embed(aspect_text)\n",
    "        aspect_len = torch.sum(aspect_text != 0, dim=-1)\n",
    "        h_t, _ = self.left_gru(aspect, aspect_len)\n",
    "        ''' Aspect term to position-aware sentence attention '''\n",
    "        alpha = F.softmax(torch.tanh(torch.add(torch.bmm(torch.matmul(h_t, self.weight_m), torch.transpose(h_x, 1, 2)), self.bias_m)), dim=1)\n",
    "        s_x = torch.bmm(alpha, h_x)\n",
    "        ''' Position-aware sentence attention to aspect term '''\n",
    "        h_x_pool = torch.unsqueeze(torch.div(torch.sum(h_x, dim=1), x_len.float().view(x_len.size(0), 1)), dim=1)\n",
    "        gamma = F.softmax(torch.tanh(torch.add(torch.bmm(torch.matmul(h_x_pool, self.weight_n), torch.transpose(h_t, 1, 2)), self.bias_n)), dim=1)\n",
    "        h_r = torch.squeeze(torch.bmm(gamma, s_x), dim=1)\n",
    "        ''' Output transform '''\n",
    "        out = torch.tanh(self.w_r(h_r))\n",
    "        out = self.w_s(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f744fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_PBAN = PBAN(embedding_matrix).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1551dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_PBAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9756fee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 1\n",
    "lr=0.001\n",
    "l2_reg=1e-5\n",
    "num_epoch = 20\n",
    "input_cols = ['text', 'aspect', 'position']\n",
    "log_step = 5\n",
    "model_name = 'pban_lstm'\n",
    "dataset = 'restaurant'\n",
    "batch_size = 64\n",
    "embed_dim = 200\n",
    "hidden_dim = 200\n",
    "polarities_dim = 3\n",
    "polarity_dict = {0: 'positive', 1: 'negative', 2:'neutral'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fff5dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "params = filter(lambda p: p.requires_grad, model_PBAN.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=lr, weight_decay=l2_reg)\n",
    "writer_PBAN = SummaryWriter(f\"runs/PBAN_LSTM/BatchSize {batch_size} LR {lr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a833d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "run(model_PBAN , writer_PBAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48db372",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0973ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ac2f01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
