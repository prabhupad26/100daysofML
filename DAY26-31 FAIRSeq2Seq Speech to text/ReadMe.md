__Speech2Text: Fine Tuning Wav2Vec2 for English Auto Speech Recognition__

**Problem Statement/Task : Generate a transcription of a voice recording which is stored in a sound file**

**Approach** :  
We will be using the base check point of a pretrained `wav2vec 2.0` ASR model which is trained on 50 hrs of unlabeled speech recordings and predicts the speaker of input speech recording. We will be adding a linear layer which will be mapping the contextual representation generated by this pretrained model to vocabulary that we have build from the dataset, this linear layer will be trained to do this mapping. 
   1. Obtain sound file to sentence data from `time_asr` dataset
   2. Remove special characters from the labels (sentences)
   3. Build a vocabulary out of all the characters in all the labels. ([UNK] , [PAD] tokens are added to the vocabs for unknown character and padding for identifying the end of words).
   4. Convert the raw sound data to sampled data which will further be used for training the model:
      - [Wav2vec2CTC tokenizer](https://huggingface.co/transformers/master/model_doc/wav2vec2.html#wav2vec2ctctokenizer) is used for tokenizing the inputs which maps the context representation created by wav2vec to the transcription based on the vocab defined in step 3.
      - Feature extractor is used with sampling rate of 16 kHz, input is also padded so that shorter input should be of same size , input is also normalized.(All the data points should have the same sampling rate).
   6. After loading the pretrained model , `require_grad` is set to False using :
      > model.freeze_feature_extractor()
   7. Model is evaluated using the [word error rate](https://huggingface.co/metrics/wer)

**Dataset used for fine-tuning:** `timit_asr` corpus containing 5300 labeled (both test-1680 and train-4620 dataset) speech of sentences recorded by [630 speakers](https://huggingface.co/datasets/timit_asr). The wav2vec2.0 model has performed the [best](https://paperswithcode.com/sota/speech-recognition-on-timit) on this dataset for a automatic speech recognition task. In this exercise we will be using this learning model to get the Text out of Speech.

**References**
1. [Fine tune Wave2vec2 for English ASR with ðŸ¤— transformer](https://huggingface.co/blog/fine-tune-wav2vec2-english)
2. [Sequence Modeling with CTC](https://distill.pub/2017/ctc/)
